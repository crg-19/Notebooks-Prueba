{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepEval demo en Binder (deepeval_demo.ipynb)\n",
        "\n",
        "Este cuaderno muestra dos rutas:\n",
        "1) **Sin API keys** (usando `JsonCorrectnessMetric`, no requiere LLM).\n",
        "2) **Con API key** (usando métricas con LLM como `AnswerRelevancyMetric` y `GEVal`).\n",
        "\n",
        "> **Sugerencia**: Si estás en Binder, este repo debe incluir un `requirements.txt` (ya preparado) para que el entorno se construya automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Instalación/Comprobación de dependencias ===\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "def pip_install(*packages):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-U', *packages])\n",
        "\n",
        "try:\n",
        "    import deepeval  # noqa\n",
        "except Exception:\n",
        "    pip_install('deepeval', 'python-dotenv')\n",
        "\n",
        "print('Entorno listo ✅')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte A · Evaluación **sin API keys** (JsonCorrectnessMetric)\n",
        "`JsonCorrectnessMetric` valida que el `actual_output` cumpla un **esquema JSON** (Pydantic). \n",
        "No utiliza un LLM, por lo que funciona sin configurar claves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pydantic import BaseModel, RootModel\n",
        "from typing import List\n",
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import JsonCorrectnessMetric\n",
        "\n",
        "# Definimos el esquema esperado\n",
        "class Item(BaseModel):\n",
        "    name: str\n",
        "    price: float\n",
        "\n",
        "class ItemList(RootModel[List[Item]]):\n",
        "    pass\n",
        "\n",
        "metric = JsonCorrectnessMetric(\n",
        "    expected_schema=ItemList,\n",
        "    include_reason=False,   # no usamos LLM para la 'reason'\n",
        "    strict_mode=True        # exige coincidencia exacta de esquema\n",
        ")\n",
        "\n",
        "# Dos casos de prueba: uno correcto y otro incorrecto\n",
        "ok_output = '[{\"name\": \"Cuaderno\", \"price\": 3.5}, {\"name\": \"Bolígrafo\", \"price\": 1.2}]'\n",
        "ko_output = '{\"name\": \"Cuaderno\", \"price\": \"tres\"}'  # NO es lista y price no es float\n",
        "\n",
        "test_cases = [\n",
        "    LLMTestCase(input='Devuélveme una lista de artículos', actual_output=ok_output),\n",
        "    LLMTestCase(input='Devuélveme una lista de artículos', actual_output=ko_output),\n",
        "]\n",
        "\n",
        "# Ejecutamos la evaluación\n",
        "_ = evaluate(test_cases=test_cases, metrics=[metric])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte B · Métricas con **LLM** (requiere API key)\n",
        "Las métricas como `AnswerRelevancyMetric` o `GEVal` usan un **LLM como juez**. \n",
        "Por defecto DeepEval utiliza modelos de OpenAI; configura `OPENAI_API_KEY` (o usa otro proveedor soportado) antes de ejecutar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "api-key"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "# OPCIONAL: Define tu API key en variables de entorno (sólo a modo de ejemplo).\n",
        "# En Binder puedes ejecutar esta celda manualmente y pegar tu clave.\n",
        "import os\n",
        "os.environ.get('OPENAI_API_KEY') or os.environ.update({'OPENAI_API_KEY': 'PON_AQUI_TU_OPENAI_API_KEY'})\n",
        "print('OPENAI_API_KEY configurada:', bool(os.environ.get('OPENAI_API_KEY')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ejemplo con AnswerRelevancyMetric + GEval\n",
        "from deepeval.metrics import AnswerRelevancyMetric, GEval\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval import evaluate\n",
        "\n",
        "answer_relevancy = AnswerRelevancyMetric(\n",
        "    threshold=0.7,\n",
        "    model='gpt-4.1',      # puedes cambiar a otro modelo soportado\n",
        "    include_reason=True,\n",
        ")\n",
        "\n",
        "correctness = GEval(\n",
        "    name='Correctness',\n",
        "    criteria=(\n",
        "        \"\"\"Determina si el 'actual output' es correcto con respecto al 'expected output'.\n",
        "        Usa criterio estricto para respuestas numéricas o hechos.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
        "    ],\n",
        "    threshold=0.5,\n",
        ")\n",
        "\n",
        "tc = LLMTestCase(\n",
        "    input=\"What if these shoes don't fit?\",\n",
        "    actual_output='We offer a 30-day full refund at no extra cost.',\n",
        "    expected_output=\"You're eligible for a free full refund within 30 days of purchase.\",\n",
        ")\n",
        "\n",
        "_ = evaluate([tc], [answer_relevancy, correctness])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Notas\n",
        "- Para ejecutar métricas con LLM **sin OpenAI**, revisa la documentación de modelos soportados (Anthropic, Gemini, Ollama, etc.) y pasa `model=...` al crear la métrica.\n",
        "- Este cuaderno está preparado para ejecutarse en Binder si tu repo incluye `requirements.txt`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}